{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Recommendation Function<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Loading-Library\" data-toc-modified-id=\"Loading-Library-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Loading Library</a></span></li><li><span><a href=\"#Function-Algorithm\" data-toc-modified-id=\"Function-Algorithm-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Function Algorithm</a></span><ul class=\"toc-item\"><li><span><a href=\"#Algorithm-Visualization\" data-toc-modified-id=\"Algorithm-Visualization-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Algorithm Visualization</a></span></li><li><span><a href=\"#Algorithm-by-Steps\" data-toc-modified-id=\"Algorithm-by-Steps-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Algorithm by Steps</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-Similarity-List\" data-toc-modified-id=\"Get-Similarity-List-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Get Similarity List</a></span></li><li><span><a href=\"#Query-Preprocessing\" data-toc-modified-id=\"Query-Preprocessing-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Query Preprocessing</a></span></li><li><span><a href=\"#Product-Matching\" data-toc-modified-id=\"Product-Matching-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Product Matching</a></span></li><li><span><a href=\"#Recommendation-Outfit\" data-toc-modified-id=\"Recommendation-Outfit-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Recommendation Outfit</a></span></li></ul></li></ul></li><li><span><a href=\"#Define-Functions\" data-toc-modified-id=\"Define-Functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Define Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Clean-Functions\" data-toc-modified-id=\"Data-Clean-Functions-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Data Clean Functions</a></span></li><li><span><a href=\"#Similarity-Calculation-Functions\" data-toc-modified-id=\"Similarity-Calculation-Functions-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Similarity Calculation Functions</a></span></li><li><span><a href=\"#Query-Preprocessing-Functions\" data-toc-modified-id=\"Query-Preprocessing-Functions-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Query Preprocessing Functions</a></span></li><li><span><a href=\"#Recommendation-Matching-Functions\" data-toc-modified-id=\"Recommendation-Matching-Functions-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Recommendation Matching Functions</a></span></li></ul></li><li><span><a href=\"#Put-into-Use\" data-toc-modified-id=\"Put-into-Use-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Put into Use</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Loading Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:24:27.904140Z",
     "start_time": "2021-05-11T19:24:14.444841Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import punkt\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import spacy\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "nlp = spacy.load('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Algorithm Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T00:11:43.459269Z",
     "start_time": "2021-05-12T00:11:43.421791Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='Algorithm.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Algorithm by Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Get Similarity List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. `calculateTFIDF`\n",
    "    - After generating all relative features, input all 60,000+ records' product_doc information, which is the product self features into TF-IDF vectorizer function `TFidfVectorizer` and get the TF-IDF weighting. This step will output the vectorizer, tf-idf table as dataframe, and the tf-idf score for further use. The vectorizer will also be used in query vectorization.  \n",
    "    \n",
    "    \n",
    "2. `word2VecWeightTFIDF`\n",
    "    - Using the output from `calculateTFIDF` and word2vec to convert product feature record documents to vectors, weighted by the tf-idf score. We limit the representing vector length as 300, so that all product features will be converted into a 300 character length vector.  \n",
    "    \n",
    "    \n",
    "3. `getProductVector`\n",
    "    - Return the list of vectors that represent each product by the order in the original dataframe.  \n",
    "    \n",
    "    \n",
    "4. `queryTFIDF`\n",
    "    - Calculate the tf-idf score for the query, output the tf-idf score for that query used in next step's word2vec weighting.  \n",
    "    \n",
    "    \n",
    "5. `queryToVecWeight`\n",
    "    - Calculate the word2vec vector of the query that is weighted by tf-idf score from former step.  \n",
    "    \n",
    "    \n",
    "6. `getQueryVector`\n",
    "    - Return the list of vectors that represent the query  \n",
    "    \n",
    "    \n",
    "7. `getSimilarity`\n",
    "    - Calculate the cosine similarity between query and each document, return the descending similarity list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Query Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. `find_query_category`\n",
    "    - We first preprocess the query and the original data to make sure their category can match to each other. Then we will find the category of the input query, if the input query describe specific category, we will recommend a product of that category  \n",
    "    \n",
    "    \n",
    "2. `find_query_brand`\n",
    "    - We first preprocess the query and the original data to make sure their brand can match to each other. Then we will find the brand of the input query, if the input query mentions specific brand, we want to return a product of that brand  \n",
    "    \n",
    "    - Here we also implemented fuzzy match to make sure if user searching for a brand that has \"\\.\" in it, for example \"J.Crew\", we can match the \"J crew\" to the actual brand name \"J.Crew\" instead of removing \"\\.\" . Thus, we choose to use fuzzy match, score for all matched tokens and take the top one, which should be the brand name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Product Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. `find_final_product_id`\n",
    "    - First we try to find the category and the brand of the query, if the query does not mention any information about category and brand, we just return the first product id in the similarity list which has the highest similarity score.\n",
    "    - If the query mention about category, but does not mention about brand, we will return the first product id in the similarity list that match the query category.\n",
    "    - If the query mention about brand, but does not mention about category, we will first search for all product that belongs to that brand and return the first product id in the similarity list that match the query brand. Also, the brand may not produce products in the category that mentioned in the query. In this situation, we will discard the brand information and return first product id in the similarity list which has the highest similarity score   \n",
    "    - If the query mention about both category and brand, we will return the first product id in the similarity list that match the query category and query brand. Also, the brand may not produce products in the category that mentioned in the query. In this situation, we will discard the brand information and return the product id only consider the brand's category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Recommendation Outfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we have the closest product that the user is looking for, we will then search for the outfit recommendations.  \n",
    "This includes two situations: If output product is in our expert outfit combination, we would return the recommended combination; else if the output product has no exist recommended outfit combination, we will search for our recommendation doc of that product and calculate similarity with all other products, and recommend the closest product combinations.\n",
    "\n",
    "1. `recommand_outfit_combinations`\n",
    "    - Find whether final product is in the outfit_combinations: If true then return the list, otherwise return []. If our final product id is in the outfit_combinations product id list, we will return the curated set of outfits picked by experts.\n",
    "    \n",
    "2. `recFind`\n",
    "    - Find recommendation similarity list: used when output product is not in our expert outfit list\n",
    "    \n",
    "3. `search_for_recommand`\n",
    "    - If our final product id is not in the existing outfit_combinations, we will search for our own recommendation. In this step, we will search for the top 3 similar products that belongs to 3 different categories.\n",
    "    \n",
    "4. `recommendation`\n",
    "    - Recommendation Function that combines previous steps\n",
    "    \n",
    "5. `getRecommendation`\n",
    "    - User used function that takes in a query and output the recommended outfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Clean Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:24:27.980018Z",
     "start_time": "2021-05-11T19:24:27.931079Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def spacyTokenizer(document):\n",
    "    '''Use spacy package to perform tokenize, remove punctuation and lemmatize the word'''\n",
    "    tokens = nlp(document)\n",
    "    tokens = [token.lemma_ for token in tokens if (\n",
    "        token.is_punct == False and \\\n",
    "        token.lemma_.strip()!= '')]\n",
    "    return tokens\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    '''function to convert nltk tag to wordnet tag'''\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    '''Use NLTK package to perform lemmatize'''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return ' '.join(lemmatized_sentence)\n",
    "\n",
    "def remove_sw(text):\n",
    "    '''Use NLTK to remove stopword'''\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk import word_tokenize\n",
    "    #using english stop word in NLTK\n",
    "    stopwords_list = stopwords.words('english')\n",
    "\n",
    "    import re\n",
    "\n",
    "    # split sentence into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    new_words = []\n",
    "    # remove stopwords\n",
    "    for w in words:\n",
    "        if w in stopwords_list:\n",
    "            continue\n",
    "        new_words.append(w)\n",
    "\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "def textClean(text):  \n",
    "    '''Lemmatize and remove srop word'''\n",
    "    return(remove_sw(lemmatize_sentence(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Similarity Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:24:27.999794Z",
     "start_time": "2021-05-11T19:24:27.982170Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calculateTFIDF(text_list):\n",
    "    '''Calculate the tf-idf given a list of documents, and return the vectorizer, tf-idf table dataframe, and the token available\n",
    "    reference: https://colab.research.google.com/drive/1CZa723-_mDmNJPG2oiExgWYF9nqbueeY?usp=sharing'''\n",
    "    from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "    #Define generate TF-idf Vectorizer for each document\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        #token_pattern=r'\\b[a-zA-Z]+\\b'\n",
    "        tokenizer = spacyTokenizer\n",
    "        #ngram_range = (1,2)\n",
    "    )\n",
    "    #fit the vectorizer to the text list\n",
    "    X = vectorizer.fit_transform(text_list)\n",
    "    #Get tf-idf table\n",
    "    tf_idf_lookup_table = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    DOCUMENT_SUM_COLUMN = \"DOCUMENT_TF_IDF_SUM\"\n",
    "    # sum the tf idf scores for each document\n",
    "    tf_idf_lookup_table[DOCUMENT_SUM_COLUMN] = tf_idf_lookup_table.sum(axis=1)\n",
    "    available_tf_idf_scores = tf_idf_lookup_table.columns \n",
    "    # a list of all the columns we have\n",
    "    available_tf_idf_scores = list(map( lambda x: x.lower(), available_tf_idf_scores)) \n",
    "    # lowercase everything\n",
    "    return vectorizer, tf_idf_lookup_table, available_tf_idf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:24:28.054560Z",
     "start_time": "2021-05-11T19:24:28.033668Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def word2VecWeightTFIDF(doc_list, tf_idf_lookup_table, available_tf_idf_scores):\n",
    "    '''Calculate the word2vec of each document and weighted by tf-idf socre\n",
    "    reference: https://colab.research.google.com/drive/1CZa723-_mDmNJPG2oiExgWYF9nqbueeY?usp=sharing'''\n",
    "    import numpy as np\n",
    "\n",
    "    doc_vectors = []\n",
    "    for idx, doc in enumerate(doc_list): # iterate through each review\n",
    "        tokens = nlp(doc) # have spacy tokenize the review text\n",
    "\n",
    "        # initially start a running total of tf-idf scores for a document\n",
    "        total_tf_idf_score_per_document = 0\n",
    "\n",
    "        # start a running total of initially all zeroes (300 is picked since that is the word embedding size used by word2vec)\n",
    "        running_total_word_embedding = np.zeros(300) \n",
    "        for token in tokens: # iterate through each token\n",
    "\n",
    "        # if the token has a pretrained word embedding it also has a tf-idf score\n",
    "            if token.has_vector and token.text.lower() in available_tf_idf_scores:\n",
    "\n",
    "                tf_idf_score = tf_idf_lookup_table.loc[idx, token.text.lower()]\n",
    "                #print(f\"{token} has tf-idf score of {tf_idf_lookup_table.loc[idx, token.text.lower()]}\")\n",
    "                running_total_word_embedding += tf_idf_score * token.vector\n",
    "\n",
    "                total_tf_idf_score_per_document += tf_idf_score\n",
    "                \n",
    "        #if no token has has a pretrained word embedding, assign 1 to aviod zero division\n",
    "        if total_tf_idf_score_per_document == 0:\n",
    "            total_tf_idf_score_per_document = 1\n",
    "\n",
    "        # divide the total embedding by the total tf-idf score for each document\n",
    "        document_embedding = running_total_word_embedding / total_tf_idf_score_per_document\n",
    "        #print(document_embedding)\n",
    "        doc_vectors.append(document_embedding)\n",
    "    return doc_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:24:28.066864Z",
     "start_time": "2021-05-11T19:24:28.058192Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getProductVector(text_df):\n",
    "    '''Return the list of vectors that reperesent each product'''\n",
    "    vectorizer, tf_idf_lookup_table, available_tf_idf_scores = calculateTFIDF(text_df)\n",
    "    print(\"TF-IDF Vectorizing Finished.\")\n",
    "    product_vectors = word2VecWeightTFIDF(text_df, tf_idf_lookup_table, available_tf_idf_scores)\n",
    "    print(\"Word2Vec  Finished.\")\n",
    "    return product_vectors, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:24:28.083993Z",
     "start_time": "2021-05-11T19:24:28.073747Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def queryTFIDF(query, vectorizer):\n",
    "    '''Calculate the tf-idf score given a query and vectorizer'''\n",
    "    query_pre_tf_idf = [textClean(query)]\n",
    "    #Clean the query\n",
    "    query_tf_idf = vectorizer.transform(query_pre_tf_idf)\n",
    "    #Calculate the score given fitted vectorizer \n",
    "    query_tf_idf_lookup_table = pd.DataFrame(query_tf_idf.toarray(), columns=vectorizer.get_feature_names())\n",
    "    #Get query tf-idf table\n",
    "    available_tf_idf_scores = query_tf_idf_lookup_table.columns \n",
    "    # a list of all the columns we have\n",
    "    available_tf_idf_scores = list(map( lambda x: x.lower(), available_tf_idf_scores)) \n",
    "    # lowercase everything\n",
    "    return query_tf_idf_lookup_table, available_tf_idf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:24:28.098333Z",
     "start_time": "2021-05-11T19:24:28.089548Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def queryToVecWeight(query, query_tf_idf_lookup_table, available_tf_idf_scores):\n",
    "    '''Calculate the word2vec of the query and weighted by tf-idf socre'''\n",
    "    query_clean = [textClean(query)]\n",
    "    #clean the query\n",
    "    return word2VecWeightTFIDF(query_clean, query_tf_idf_lookup_table, available_tf_idf_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:24:28.111737Z",
     "start_time": "2021-05-11T19:24:28.104036Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getQueryVector(query, vectorizer):\n",
    "    '''Return the list of vectors that reperesent the query'''\n",
    "    query_tf_idf_lookup_table, available_tf_idf_scores = queryTFIDF(query, vectorizer)\n",
    "    query_vectors = queryToVecWeight(query, query_tf_idf_lookup_table, available_tf_idf_scores)\n",
    "    return query_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:24:28.123454Z",
     "start_time": "2021-05-11T19:24:28.115422Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSimilarity(total_doc, query_vector, index_name):\n",
    "    '''Calculate the cosine similarity between query and each document, return the descending similarity list'''\n",
    "    similarities = pd.DataFrame(cosine_similarity(total_doc, query_vector), columns=['similarity'], index=index_name)\n",
    "    #Calculate the cosine similarity\n",
    "    return similarities['similarity'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:24:28.159224Z",
     "start_time": "2021-05-11T19:24:28.127810Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_query_category(txt): \n",
    "    '''find the category of the input query, if the input query describe specific category, we want to recommend a product of that category'''\n",
    "    \n",
    "    # define the pattern for each category\n",
    "    top_re=r'\\b(top|shirt|tee|blouse|tank|tshirt|knitwear|vest|camisole|overshirt|long sleeve|v neck|tunic|turtleneck|shell|sleeve|polo|cami|slvls|vneck|shrt|pckt t|crew|sleeveless|sweatshirt|longsleeve|hoodie|pullover|sweat|crewneck|pullovr|swtshrt)\\b'\n",
    "    outwear_re = r'\\b(jacket|coat|blazer|outerwear|trench|parka|anorak|puffer|overcoat|topcoat|bomber|trucker|jckt|blazr|jkt)\\b'\n",
    "    bottom_re = r'\\b(jean|pant|jogger|skirt|short|trouser|legging|leg|skort|bottom|low straight|loose straight|straight fit|loose fit|relax straight|relaxed straight|vintage straight|slim taper|sweatpants?|miniskirt|culotte|bootcut|dojo|crop|skinny|stretch denim|slimmy|high rise|low rise|mid rise|garment|paxtyn|ankle|denim slimmy|high waist|flare|skny|exaggerate kick)\\b'\n",
    "    shoe_re = r'\\b(sandal|booty|shoe|sneaker|boot|pump|flat|heel|bootie|mule|espadrille|oxford|loafer|slipper|flip flop|clog|derby|slip on|wedge|slide|nappa|leather upper)\\b'\n",
    "    sweater_re=r'\\b(sweater|cardigan|knitwear|swtr)\\b'\n",
    "    onepiece_re = r'\\b(dress|jumpsuit|shirtdress|dressesandjumpsuits?|bodysuit|housedress|rompers?|kimono|robe|one piece|onepiece|minidress|denim set|pj set|short set|lounge set|sleep set|cami set|trouser set|pajama set|pc set|bra set|slipdress|pantsuit|shirtdress|raincoat|caftan|suit|slip|jump|jumpsut)\\b'\n",
    "    accessory_re = r'\\b(bags?|tote|crossbody|handbags?|belt|tie|accessory|ladybag|backpack|gymsack|cardholder|glove|wristlet|sunglass|satchel|leash|laptop|camera|jewelry|wallet|eyewear|scarf|scarves|clutch|poncho|calendar|bangle|keychain|mat|postcard|tumbler|pouch|hobo|hat|earring|barrette|haircalf|box|cape|mask|mitt|sock|wrap|jasper|bracelet|necklace|scrunchie|biker|millinery|card|fedora|felt|pack|napkin|shawl|hook|chain|spray|diffuser|brim|cap|blanket|josefina|bottle|basket|eye|extérieure|paint|mousse|glass|collection|acrylic|accessory|clip|beanie|beret)\\b'\n",
    "    underwear_re = r'\\b(bra|underwear|bralett|bralette|thong|panty|underwire|brakini)\\b'\n",
    "    othercloth_re=r'\\b(wears?|classic fit|regular fit|denim|cloth|othercloth)\\b'\n",
    "    home_re=r'\\b(candle|vase|mug|pot|chair|mist|towel|lamp|glasswear|plate|pillow|table|coverlet|sofa|bed|desk|bowl|dish|bath|jar|soap|handwoven|chèvre|wash|home)\\b'\n",
    "    cosmetics_re=r'\\b(sanitizer|cream|lip|oil|perfume|perfum|nail)\\b'\n",
    "    book_re=r'\\b(books?|lookbook|workbook)\\b'\n",
    "    archive_re=r'\\barchive\\b'\n",
    "\n",
    "    # use regular expression to find the brand of the query\n",
    "    # if we cannot find a category for the query, we put 'unknow'\n",
    "    txt = str(txt)\n",
    "    val = np.nan\n",
    "    if re.search(shoe_re, txt, re.IGNORECASE ):\n",
    "        val = \"shoe\"\n",
    "    elif re.search(onepiece_re, txt, re.IGNORECASE ):\n",
    "        val = \"onepiece\"    \n",
    "    elif re.search(bottom_re, txt, re.IGNORECASE ):\n",
    "        val = \"bottom\"\n",
    "    elif re.search(top_re, txt, re.IGNORECASE ):\n",
    "        val = \"top\"\n",
    "    elif re.search(outwear_re, txt, re.IGNORECASE ):\n",
    "        val = \"outwear\"\n",
    "    elif re.search(sweater_re, txt, re.IGNORECASE ):\n",
    "        val = \"sweater\"   \n",
    "    elif re.search(underwear_re, txt, re.IGNORECASE ):\n",
    "        val = \"underwear\"\n",
    "    elif re.findall(othercloth_re, txt, re.IGNORECASE ):\n",
    "        val = \"othercloth\"\n",
    "    elif re.search(accessory_re, txt, re.IGNORECASE ):\n",
    "        val = \"accessory\"\n",
    "    elif re.search(home_re, txt, re.IGNORECASE ):\n",
    "        val = \"home\"\n",
    "    elif re.search(cosmetics_re, txt, re.IGNORECASE ):\n",
    "        val = \"cosmetics\"\n",
    "    elif re.search(book_re, txt, re.IGNORECASE ):\n",
    "        val = \"book\"\n",
    "    elif re.search(archive_re, txt, re.IGNORECASE ):\n",
    "        val = \"archive\"\n",
    "    else:\n",
    "        val = \"unknow\"\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:24:28.173853Z",
     "start_time": "2021-05-11T19:24:28.162806Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_query_brand(txt):\n",
    "    '''find the brand of the input query, if the input query mentions specific brand, we want to return a product of that brand'''\n",
    "    txt = str(txt)\n",
    "    brand_list = list(data['brand'].unique())\n",
    "    # define the brand pattern\n",
    "    brand_re=r'\\b(Two|Collina Strada|Cariuma|Maia Bergman|Misa|Banjanan|Rails|Rachel Comey|Nomasei|Aera|AG|Prism|Monogram|Frame|Tibi|EMME PARSONS|Studio 189|Joie|Staud|Vince|Avavav|ROTATE|Batsheva|Chufy|Gigi Burris|Warm|Holden|Nili Lotan|Simon Miller|Jonathan Cohen|Hyer Goods|Whit|Christina Lehr|Tanya Taylor|Behold Kim\\'s Dresses|G. Label|G. Sport|Theory|Wandler|Goldsign|MadeWorn|St. Agni|BERNARDO|Cult Gaia|Equipment|La DoubleJ|Jimmy Choo|Zimmermann|ATP Atelier|Helmut Lang|Acne Studios|Axel Arigato|Dr. Martens|Mara Hoffman|Nobody Denim|Ulla Johnson|ANINE BING|GRLFRND Denim|MOTHER|Studio Amelia|Alo Yoga|Little Liffner|LoveShackFancy|We Are Kindred|Jenni Kayne|Loeffler Randall|Stuart Weitzman|Alexandre Birman|Varley US|Significant Other|I Love Mr. Mittens|Isabel Marant Étoile|Manolo Blahnik|Nicholas Kirkwood|THE OFFICE OF ANGELA SCOTT|Eleven Six|Araks|lemlem|Janessa Leone|Sea|7 For All Mankind|Re/done|BROCHU WALKER|Sandy Liang|Kin the Label|Clare V.|Jason Scott|Andrea Iyamah|6397|Cynthia Rowley|Triarchy|Intentionally Blank|ALO|LTH JKT|Ciao Lucia|Nike|Nu Swim|Lowercase|ASTR the Label|Mother Denim|Trovata|Cesta Collective|Aesther Ekme|Mari Giudicelli|St. Roche|CQY|Illesteva|A.L.C.|House of Fluff|Cara Cara|E.L.V. Denim|Nuance|Les Girls Les Boys|Urban Savage|Either / Or|Merlette|Esquivel|LE SUPERBE|Alumnae|Zonarch|M.M.LaFleur|Veja|Gigi New York|Alexander Wang|Ancient Greek Sandals|Faherty|Outerknown|Want Les Essentiels|Tkees|Matthew Bruch|Gray Matters|Vaara|Vee Collective|Tallulah & Hope|Karla Colletto|Mackage|Behold Demo Shop (Shopify)|Demellier|AMO|Co|YCH|AMUR|Khaite|LOQ|Marni|Rhode|RtA|Acler|Akris|Anaak|Atoir|BOSS|Ellery|Ganni|Haight|J.Crew|Loewe|MONSE|Prada|ROAN|Skin|USISI|WONE|Xirena|A Emery|AMIRI|Agolde|Alexis|Asceno|Attico|Bassike|Bourie|By Far|Chloé|DÔEN|Gucci|Khaore|Koral|MATIN|Nanushka|Partow|REISS|TOD\\'S|Toteme|Youser|aeydē|Alanui|Allude|Anemone|Cefinn|Chylak|Deveaux|JORDAN|Joseph|LEGRES|Matteau|WILLOW|Yuzefi|Alex Mill|Altuzarra|Anna Sui|Cushnie|Givenchy|Halston|J Brand|K Jacques|Max Mara|Nagnata|The Row|Alix NYC|Anna Quan|Aquazzura|Bouguessa|CASASOLA|Cami NYC|Derek Lam|Eve Denim|Jacquemus|L\\'Agence|La Ligne|Le Kasha|MADEWELL|MATÉRIEL|Nicholas|we11done|ALLSAINTS|Alex Perry|Aquatalia|Baserange|Handvaerk|Pippa Holt|Retrofête|Stateside|Stine Goya|The Range|salondeju|Apiece Apart|Balenciaga|Danse Lente|Dolce Vita|L.F.Markey|PAUL GREEN|THE GREAT.|Tory Burch|Tory Sport|ZELLA BODY|rag & bone|Albus Lumen|Amina Muaddi|Anna October|Birkenstock|COACH|Dodo Bar Or|Free People|Golden Goose|James Perse|LOW CLASSIC|Manu Atelier|Marika Vera|Marissa Webb|PatBO|Petar Petrov|Raffi|Reformation|Sacai|Sam Edelman|Sies Marjan|Sole Society|Blazé Milano|Carrie Forbes|Envelope1976|FRANCO SARTO|Fleur du Mal|Frankie Shop|Isabel Marant|JOSEF SEIBEL|Norma Kamali|Orseund Iris|Prabal Gurung|REDValentino|Rosetta Getty|Schutz|Stand Studio|VINCE CAMUTO|Veronica Beard|Zeynep Arçay|CHARLES DAVID|Common Projects|George Keburia|H Brand|Hunting Season|Laurence Dacade|Mansur Gavriel|Saint Laurent|Sally LaPointe|See By Chloé|Self-Portrait|Solace London|The Line By K|Vanessa Bruno|Wolford|A.W.A.K.E. MODE|Alessandra Rich|Alix of Bohemia|Andersson Bell|Aviator Nation|Banana Republic|Bottega Veneta|Converse|Cotton Citizen|Dolce & Gabbana|FREDA SALVADOR|Gianvito Rossi|Lado Bokuchava|Live the Process|Proenza Schouler|Raquel Allegra|Rosie Assoulin|Victoria Beckham|Alberta Ferretti|Blank NYC|Brock Collection|Carolina Herrera|Christopher Kane|Cole Haan|ENGLISH FACTORY|Emilia Wickstead|Gabriela Hearst|LE 17 SEPTEMBRE|Mother of Pearl|Solid & Striped|Tabitha Simmons|alexanderwang.t|King & Tuckfield|Lauren Manoogian|PROSPERITY DENIM|Stella McCartney|TWENTY Montréal|ZADIG & VOLTAIRE|adidas Originals|Akris punto|Alexander McQueen|Alexandre Vauthier|Ann Demeulemeester|Brunello Cucinelli|Charlotte Pringels|Elie Tahari|Giambattista Valli|Jennifer Chamandi|Madeleine Thompson|Marc Jacobs|Mercedes Castillo|Sapto Djojokartiko|360 Cashmere|Canada Goose|Citizens of Humanity|Dorothee Schumacher|Faithfull The Brand|Salvatore Ferragamo|Baum und Pferdgarten|Christian Louboutin|Eileen Fisher|MM6 Maison Margiela|ST. JOHN COLLECTION|Alice + Olivia|MICHAEL Michael Kors|Nancy Gonzalez|Yigal Azrouël|n:Philanthropy|McQ Alexander McQueen|Rebecca Minkoff|SARTO BY FRANCO SARTO|Charlotte Simone|Mestiza New York|Polo Ralph Lauren|adidas by Missoni|Victoria Victoria Beckham|ATM Anthony Thomas Melillo|Herschel Supply Co.|Off-White c/o Virgil Abloh|REMAIN Birger Christensen|kate spade new york|Aleksandre Akhalkatsishvili|Donna Karan New York|TAILORED BY REBECCA TAYLOR|Philosophy di Lorenzo Serafini)\\b'\n",
    "    # use regular expression to find the brand of the query\n",
    "    # if the query does not mention any brand, we leave the brand of the query as empty list\n",
    "    brand = re.findall(brand_re, txt, re.IGNORECASE)\n",
    "    if brand != []:\n",
    "        brand = process.extractOne(brand[0][0], brand_list)[0] \n",
    "        #match the closest brand name if there is a typo using fuzzy match\n",
    "    else:\n",
    "        brand = 'unknow'\n",
    "    return brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:24:28.197560Z",
     "start_time": "2021-05-11T19:24:28.176261Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_final_product_id(similarity_list,query):\n",
    "    '''find the final product id'''\n",
    "    # find the category and the brand of the query\n",
    "    query_category = find_query_category(query)\n",
    "    query_brand = find_query_brand(query)\n",
    "    \n",
    "    # if the query does not mention any information about category and brand,\n",
    "    # we just return the first product id in the similarity list\n",
    "    if query_category == 'unknow' and query_brand == 'unknow':\n",
    "        return similarity_list.index[0]\n",
    "    \n",
    "    # else if the query mention about category, but does not mention about brand,\n",
    "    # we return the first product id in the similarity list that match the query category\n",
    "    elif query_category != 'unknow' and query_brand == 'unknow':\n",
    "        for i in similarity_list.index:\n",
    "        #inter through the similarity list\n",
    "            item_category = data[data['product_id'] == i]['general_category'].values[0]\n",
    "            # find the category of this product\n",
    "            if item_category == query_category:\n",
    "            #check if match the query category, if true then return the product id\n",
    "                return i\n",
    "            \n",
    "    # else if the query mention about brand, but does not mention about category,\n",
    "    # we return the first product id in the similarity list that match the query brand\n",
    "    elif query_category == 'unknow' and query_brand != 'unknow':\n",
    "        brand_data = data[data['brand'] == query_brand].iloc[:,:4]\n",
    "        #filter the data to get the subset with the same brand mentioned in the query\n",
    "        merge_data = brand_data.merge(similarity_list, on = 'product_id', how = 'inner')\n",
    "        #merge the to get the similarity list with only the brand mentioned in the query\n",
    "        \n",
    "        if len(merge_data) == 0:\n",
    "        #If the brand does produce the product in this category, that we can not find the inersection set with brand and category\n",
    "            return similarity_list.index[0]\n",
    "            #discard the brand info only return the most similar one\n",
    "        else:\n",
    "            return merge_data.sort_values(by = 'similarity', ascending = False).loc[0,'product_id']\n",
    "            #return the product in the same brand with the query mentioned\n",
    "    \n",
    "    # else if the query mention about both category and brand\n",
    "    # we return the first product id in the similarity list that match the query category and query brand\n",
    "    else:\n",
    "        brand_data = data[data['brand'] == query_brand].iloc[:,:4]\n",
    "        #filter the data to get the subset with the same brand mentioned in the query\n",
    "        merge_data = brand_data.merge(similarity_list, on = 'product_id', how = 'inner')\n",
    "        #merge the to get the similarity list with only the brand mentioned in the query\n",
    "        merge_data_category_list = merge_data['general_category'].unique()\n",
    "        #get the total category list\n",
    "        if query_category not in merge_data_category_list:\n",
    "        #If the brand does produce the product in this category\n",
    "            for i in similarity_list.index:\n",
    "            #iter through the list and find the first match the category mentioned in the query\n",
    "                item_id = similarity_list.loc[i,'product_id']\n",
    "                item_category = data[data['product_id'] == item_id]['general_category'].values[0]\n",
    "                if item_category == query_category:\n",
    "                    return item_id\n",
    "        else:\n",
    "        #If the brand produce the product in the category query mentioned\n",
    "            sort_merge_data = merge_data.sort_values(by = 'similarity', ascending = False)\n",
    "            #Get the descending the similarity list\n",
    "            for i in sort_merge_data.index:\n",
    "            #iter throught the list find the first product match the category mentioned in the query\n",
    "                item_id = sort_merge_data.loc[i,'product_id']\n",
    "                item_category = data[data['product_id'] == item_id]['general_category'].values[0]\n",
    "                if item_category == query_category:\n",
    "                    return sort_merge_data.loc[i,'product_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation Matching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:24:28.212423Z",
     "start_time": "2021-05-11T19:24:28.200107Z"
    }
   },
   "outputs": [],
   "source": [
    "def recommend_outfit_combinations(final_id):\n",
    "    '''find whether final product is in the outfit_combinations, Ture return the list, false return []'''\n",
    "    product_id_list = list(outfit_combinations['product_id'])\n",
    "    #Get the product id that has outfit conbination in the data\n",
    "    \n",
    "    result_dict = {}\n",
    "    # If the outfit id is in the outfit combination dataset, we will look up the dataset and return the recommended outfit\n",
    "    if final_id in product_id_list:\n",
    "        outfit_list = list(outfit_combinations[outfit_combinations['product_id'] == final_id]['outfit_id'])\n",
    "        #get all the outfit_id that contains the product\n",
    "        final_outfit_id = np.random.choice(outfit_list)\n",
    "        #if the random choice one outfit_id \n",
    "        recommend_df = outfit_combinations[outfit_combinations['outfit_id'] == final_outfit_id]\n",
    "        #Get the outfit conbination with this outfit_id\n",
    "        for i in range(len(recommend_df)):\n",
    "        #save the outfit conbination in the dictionary for return\n",
    "            result_dict[recommend_df.iloc[i,2]] = recommend_df.iloc[i,4] + \"(\" + recommend_df.iloc[i,1] + \")\"\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:24:28.225767Z",
     "start_time": "2021-05-11T19:24:28.215741Z"
    }
   },
   "outputs": [],
   "source": [
    "def recFind(final_id):\n",
    "    '''find recomendation similarity list'''\n",
    "    query = data[data['product_id'] == final_id]['recommendation_doc'].values[0]\n",
    "    \n",
    "    # Find the query vector\n",
    "    query_vectors = getQueryVector(query, vectorizer)\n",
    "    \n",
    "    # Calculate the similarity of the query using the product vectors\n",
    "    similarity_list = getSimilarity(product_vectors, query_vectors, product_id_index)\n",
    "    return similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T23:11:13.707142Z",
     "start_time": "2021-05-11T23:11:13.660621Z"
    }
   },
   "outputs": [],
   "source": [
    "def search_for_recommend(final_product_id,similarity_list_rec):\n",
    "    '''search for the top 3 similar products that belongs to 3 different categories.'''\n",
    "    final_category = data[data['product_id'] == final_product_id]['general_category'].values[0]\n",
    "    \n",
    "    # Make sure if asking for top and bottom, it will not return product from onepiece category\n",
    "    # If ask for onepiece, it will not return product from to or bottom category\n",
    "    non_pick_list = []\n",
    "    if final_category in ['top','bottom']:\n",
    "        non_pick_list = ['onepiece']\n",
    "    elif final_category == 'onepiece':\n",
    "        non_pick_list = ['top','bottom']\n",
    "    \n",
    "    final_category_list = []\n",
    "    final_category_list.append(final_category)\n",
    "\n",
    "    final_id_list = []\n",
    "    final_id_list.append(final_product_id)\n",
    "\n",
    "    # Check from the similarity list, return the top score item that belongs to a new category\n",
    "    # return the top 4 categories item\n",
    "    for i in similarity_list_rec.index:\n",
    "        recom_set = []\n",
    "        item_category = data[data['product_id'] == i]['general_category'].values[0]\n",
    "        if item_category in final_category_list:\n",
    "            continue\n",
    "        elif item_category in non_pick_list:\n",
    "            continue\n",
    "        else:\n",
    "            final_category_list.append(item_category)\n",
    "            final_id_list.append(i)\n",
    "        if len(final_category_list) >= 4:\n",
    "            break\n",
    "    \n",
    "    result_dict = {}\n",
    "    for return_id in final_id_list:\n",
    "        return_data = data[data['product_id'] == return_id]\n",
    "        return_category = return_data['general_category'].values[0]\n",
    "        return_detailed = return_data['name'].values[0]\n",
    "        result_dict[return_category] = return_detailed + \"(\" + return_id + \")\"\n",
    "        \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:24:28.255267Z",
     "start_time": "2021-05-11T19:24:28.248209Z"
    }
   },
   "outputs": [],
   "source": [
    "def recommendation(query, similarity_list):\n",
    "    '''Recommendation Function that combines previous steps'''\n",
    "    final_id = find_final_product_id(similarity_list,query)\n",
    "    \n",
    "    result1 = recommend_outfit_combinations(final_id)\n",
    "    \n",
    "    # If the product has expert outfit recommendation, use result1\n",
    "    if result1 != {}:\n",
    "        return result1\n",
    "    \n",
    "    # If the product has no expert outfit recommendation, calculate the similarity first and use the algorithm in\n",
    "    # search_for_recommend to find the recommended outfit combination\n",
    "    similarity_list_rec = recFind(final_id)\n",
    "        \n",
    "    result2 = search_for_recommend(final_id,similarity_list_rec)\n",
    "    return result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Put into Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:24:28.965628Z",
     "start_time": "2021-05-11T19:24:28.258391Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "data = pd.read_csv('final_feature.csv')\n",
    "outfit_combinations = pd.read_csv('outfit_combinations USC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T20:32:58.523140Z",
     "start_time": "2021-05-11T19:24:28.971943Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Settings\n",
    "text_df = list(data['product_doc'].values)\n",
    "product_id_index = data['product_id']\n",
    "\n",
    "#Get 300 num, and tfidf vectorizer, for each product, run before function, only one time need\n",
    "product_vectors, vectorizer = getProductVector(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T20:32:58.555732Z",
     "start_time": "2021-05-11T20:32:58.528976Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getRecommendation(query):\n",
    "    '''Final User used function that takes in a query and output the recommended outfit'''\n",
    "    query_vectors = getQueryVector(query, vectorizer)\n",
    "    similarity_list = getSimilarity(product_vectors, query_vectors, product_id_index)\n",
    "    rec_dict = recommendation(query, similarity_list)\n",
    "    return rec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T23:07:39.145986Z",
     "start_time": "2021-05-11T23:07:37.384484Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "query = 'lim fitting, straight leg pant with a center back zipper and slightly cropped leg'\n",
    "getRecommendation(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T23:14:08.990194Z",
     "start_time": "2021-05-11T23:14:06.996136Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "query = 'summer pink silk dress'\n",
    "getRecommendation(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Recommendation Function",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
